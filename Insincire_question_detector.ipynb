{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\n#import libaries for word tokenizing\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nMAX_NB_WORDS = 20000","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a8ce6b6aaaddcd9922d3479701d44e9e0ddb1c81"},"cell_type":"markdown","source":"Reading the data"},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv('../input/train.csv',usecols=[1, 2])\nX = train_data.iloc[:,0]\ny = train_data.iloc[:,1]\n\nSEQ_LEN = int(X.str.len().quantile(0.7))","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cd75d06678f201b6dc312f45781ad8e47c3f09fc"},"cell_type":"markdown","source":"Tokenizing and splitting the data"},{"metadata":{"trusted":true,"_uuid":"680f1405b8ad97a189bd1207cc8dbb5a20a3fd3c"},"cell_type":"code","source":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(X)\nsequences = tokenizer.texts_to_sequences(X)\n\nword_index = tokenizer.word_index\nX = pad_sequences(sequences, maxlen=SEQ_LEN)\nX[0]","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"1e00024ccb50286d14845d37ce22a86bf6ba519e"},"cell_type":"markdown","source":"Splitting the data to train and valid"},{"metadata":{"trusted":true,"_uuid":"8bce5df1a6015a6cbe6add017714f78535b0decc"},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(42)  # fix the randomness for reproducibility\nX_train, X_valid,y_train, y_valid = train_test_split(X, y, \n                                                    test_size=0.2, \n                                                    random_state=42)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"07a38bc372f524235e85a7379fb05d016180b65c"},"cell_type":"markdown","source":"Importing the embedding matrix"},{"metadata":{"trusted":true,"_uuid":"d4bccbceefc7ba9a34f52703c97959a2a4d28ea7"},"cell_type":"code","source":"embeddings_index = {}\nf = open(os.path.join(\"../input/embeddings/glove.840B.300d\", 'glove.840B.300d.txt'))\nfor line in f:\n    values = line.split()\n    word = \"\".join(values[:-300])\n    coefs = np.asarray(values[-300:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"161cc390eef58ca854c7056654cbc5e5b9498b5c"},"cell_type":"code","source":"embedding_matrix = np.zeros((len(word_index) + 1, 300))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"2783d76aab31153556a01191382b4b5ad13ca217"},"cell_type":"markdown","source":"Creating a f1 metric class, used to evaluate the f1 score after each epoch"},{"metadata":{"trusted":true,"_uuid":"8951ff99e263c3c756634779d409b6334d984633"},"cell_type":"code","source":"from sklearn.metrics import f1_score\n\n\ndef my_metric(y_true, y_pred):\n     return f1_score(y_true, y_pred)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"a2f679a9b1ec929ecc77c09f48e047a3f7c0ac0d"},"cell_type":"markdown","source":"Now creating the LSTM"},{"metadata":{"trusted":true,"_uuid":"7e9be94483a5a7987bbceaa44a3ee69bd959f5b0"},"cell_type":"code","source":"from keras.layers import Embedding\nfrom keras.layers import Input, Dense, Dropout, Embedding, CuDNNLSTM, Flatten, Bidirectional, Dropout, GlobalMaxPool1D\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nfrom keras.optimizers import Adam\n\ndef create_model(params_list, num_of_sampling, X, y, num_of_epoches):\n    inputs = Input(shape=(SEQ_LEN, ))\n    #this is embedding layer with randomized inital weights\n    # embedding_layer = Embedding(len(word_index) + 1,\n    #                             300,\n    #                             input_length=SEQ_LEN)(inputs)\n    #uncomment this to use the embedding matrix\n\n    embedding_layer = Embedding(len(word_index) + 1,\n                                300,\n                                weights=[embedding_matrix],\n                                input_length=SEQ_LEN,\n                               trainable=False)(inputs)\n\n    x = Bidirectional(CuDNNLSTM(params_list[0], return_sequences=True))(embedding_layer)\n    x = Bidirectional(CuDNNLSTM(params_list[1], return_sequences=True))(x)\n    x = GlobalMaxPool1D()(x)\n    x = Dense(params_list[2], activation='relu')(x)\n    x = Dropout(0.2)(x)\n    x = Dense(params_list[3], activation='relu')(x)\n    predictions = Dense(1, activation='sigmoid')(x)\n    adam = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n    model = Model(inputs=[inputs], outputs=predictions)\n    model.compile(optimizer='adam',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n    model.summary()\n    #filepath=\"weights.hdf5\"\n    \n    #checkpointer = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n    earlystopping = EarlyStopping(monitor='val_loss',\n                              min_delta=0,\n                              patience=1,\n                              verbose=0, mode='auto')\n    history = model.fit(X[:num_of_sampling], y[:num_of_sampling], \n                        validation_data=(X_valid[:num_of_sampling], y_valid[:num_of_sampling]),\n                        batch_size=200, verbose=1, \n                        shuffle=True, epochs=num_of_epoches, callbacks=[earlystopping])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"cdf24f5ee61c76731d22a74862ba3952c6530610"},"cell_type":"markdown","source":"Cumputes the best f1 score for each model, with the best treshold "},{"metadata":{"trusted":true,"_uuid":"161550d86d6afd2bb50cfe6fea7f4c2521e96043"},"cell_type":"code","source":"\ndef compute_f1(model):\n    predicted = model.predict(X_valid)\n    f1_best = 0\n    for treshold in np.linspace(0.1,0.55,15):  # beacuse we use a sigmoid function, we can try diffrent threshold to determine 0 or 1 in predictions\n        predicted[predicted < treshold] = 0\n        predicted[predicted != 0] = 1\n        f1_temp = f1_score(y_valid, predicted)\n        if f1_temp > f1_best:\n            f1_best = f1_temp\n            best_tresh = treshold\n    return f1_best, best_tresh","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f502b03364d796d60c4535843a1eb6e509c4e8d7"},"cell_type":"markdown","source":"Creating random grid search in order to find the best parameters - optional, unmark to use\n"},{"metadata":{"trusted":true,"_uuid":"abec58c4f915adabbe38f16550025d822230bf7f"},"cell_type":"markdown","source":"def random_grid_search(params_list_of_lists, num_of_searches):\n    counter = 0 # use to contorl the number of searches\n    parameters_list = [] # list of parameters we already tried\n    best_f1=0\n    while counter < num_of_searches:\n        final_params = [np.random.choice(param_list, 1)[0] for param_list in params_list_of_lists]\n        if final_params in parameters_list:  # if we already randomed this combination of parameters, try again\n            continue\n        parameters_list.append(final_params)\n        print(f\"iter num #{counter}\")\n        f1_temp, tresh = compute_f1(create_model(final_params, X=X_train, y=y_train, num_of_sampling=100000))\n        if f1_temp > best_f1:\n            best_f1 = f1_temp\n            best_tresh = tresh\n            best_params = lst\n        counter += 1\n    return best_params, best_f1, best_tresh"},{"metadata":{"trusted":true,"_uuid":"dd69bc24fab2d4425f24e1d4acde9467fc7d324a"},"cell_type":"markdown","source":"Training the model "},{"metadata":{"trusted":true,"_uuid":"27d2c7d04bf260b66c1dd98681e2e99e8137071b"},"cell_type":"code","source":"model = create_model(params_list= [64, 64, 30, 200, 'adam'], num_of_sampling=-1, X=X, y=y, num_of_epoches=20)\n","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"bec2997a1a56ffa3985165c3e655e08bf18763bc"},"cell_type":"markdown","source":"Preparing the test data for prediction"},{"metadata":{"trusted":true,"_uuid":"63deb3deef4180c4375f49f0ede035048ad8f654"},"cell_type":"code","source":"test_data = pd.read_csv('../input/test.csv')\nX_test = test_data.question_text.values\nsequences = tokenizer.texts_to_sequences(X_test)\nword_index = tokenizer.word_index\nX_test = pad_sequences(sequences, maxlen=SEQ_LEN)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8bfe8a66afacf3bbfe1186c973c30e16221ed01b"},"cell_type":"markdown","source":"Finding the best treshold"},{"metadata":{"trusted":true,"_uuid":"34799013e8d7bd5ec3bd3a8e5ecfe9d1f5346015"},"cell_type":"code","source":"#f1_best, best_tresh = compute_f1(model)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"6ae53f32ebd2b55b2c212fa69a742ed5b47260c4"},"cell_type":"markdown","source":"Now creating the predictions in order to submit the results"},{"metadata":{"trusted":true,"_uuid":"77aa048f0d37218d069313518ce7c886168d5cde"},"cell_type":"code","source":"y_pred = model.predict(X_test)\ny_pred[y_pred < 0.3] = 0\ny_pred[y_pred != 0] = 1\ny_pred = y_pred.astype(int)\nfinal_submission = test_data.drop(['question_text'], axis=1)\nfinal_submission['prediction'] = y_pred\nfinal_submission.to_csv(\"submission.csv\", index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6f83761f5e55cfa45cf00f5464ccf5ec40f16919"},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"60a95c6c5595b07c2808d0a7da3a5caad55b449e"},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}